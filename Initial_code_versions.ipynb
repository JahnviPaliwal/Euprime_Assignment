{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE\n",
        "### *This is a rough notebook, showcasing all the work done before to finalize the working code.*"
      ],
      "metadata": {
        "id": "5lu5Bhi6bczh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNifxCaxyiOI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def scrape_linkedin(search_query):\n",
        "    url = f\"https://www.linkedin.com/search/results/people/?keywords={search_query}\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        names = soup.find_all('span', class_='name')\n",
        "        job_titles = soup.find_all('div', class_='job-title')\n",
        "        leads = []\n",
        "\n",
        "        for name, title in zip(names, job_titles):\n",
        "            leads.append({\n",
        "                \"name\": name.text.strip(),\n",
        "                \"title\": title.text.strip(),\n",
        "            })\n",
        "\n",
        "        return leads\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "\n",
        "search_query = \"Director of Toxicology\"\n",
        "leads = scrape_linkedin(search_query)\n",
        "print(leads)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from Bio import Entrez\n",
        "import re\n",
        "\n",
        "\n",
        "Entrez.email = \"\"\n",
        "\n",
        "\n",
        "def scrape_pubmed_and_conferences(job_titles, publication_keywords, conference_urls, max_results=20):\n",
        "    leads = []\n",
        "\n",
        "\n",
        "    query = \" AND \".join(publication_keywords)\n",
        "\n",
        "    handle = Entrez.esearch(\n",
        "        db=\"pubmed\",\n",
        "        term=query,\n",
        "        retmax=max_results\n",
        "    )\n",
        "    record = Entrez.read(handle)\n",
        "    pubmed_ids = record[\"IdList\"]\n",
        "\n",
        "    if pubmed_ids:\n",
        "        fetch = Entrez.efetch(\n",
        "            db=\"pubmed\",\n",
        "            id=\",\".join(pubmed_ids),\n",
        "            rettype=\"medline\",\n",
        "            retmode=\"text\"\n",
        "        )\n",
        "\n",
        "        articles = fetch.read().split(\"\\n\\n\")\n",
        "\n",
        "        for article in articles:\n",
        "            author_match = re.search(r\"AU  - (.+)\", article)\n",
        "            title_match = re.search(r\"TI  - (.+)\", article)\n",
        "            aff_match = re.search(r\"AD  - (.+)\", article)\n",
        "\n",
        "            if author_match and title_match:\n",
        "                affiliation = aff_match.group(1) if aff_match else \"\"\n",
        "\n",
        "\n",
        "                if any(jt.lower() in affiliation.lower() for jt in job_titles):\n",
        "                    leads.append({\n",
        "                        \"name\": author_match.group(1),\n",
        "                        \"title\": next((jt for jt in job_titles if jt.lower() in affiliation.lower()), \"Researcher\"),\n",
        "                        \"publication_or_event\": title_match.group(1),\n",
        "                        \"source\": \"pubmed\",\n",
        "                        \"url\": \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
        "                    })\n",
        "\n",
        "\n",
        "    for url in conference_urls:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code != 200:\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "\n",
        "            people_blocks = soup.find_all(\n",
        "                [\"div\", \"li\", \"section\"],\n",
        "                class_=re.compile(\"speaker|author|presenter|faculty\", re.I)\n",
        "            )\n",
        "\n",
        "            for block in people_blocks:\n",
        "                text = block.get_text(\" \", strip=True)\n",
        "\n",
        "                if any(jt.lower() in text.lower() for jt in job_titles):\n",
        "                    leads.append({\n",
        "                        \"name\": text.split(\",\")[0],\n",
        "                        \"title\": next((jt for jt in job_titles if jt.lower() in text.lower()), \"\"),\n",
        "                        \"publication_or_event\": soup.title.text if soup.title else \"\",\n",
        "                        \"source\": \"conference\",\n",
        "                        \"url\": url\n",
        "                    })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "    return leads\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# job_titles = [\n",
        "#     \"Director\",\n",
        "#     \"Professor\",\n",
        "#     \"Principal Scientist\"\n",
        "# ]\n",
        "\n",
        "# publication_keywords = [\n",
        "#     \"toxicology\",\n",
        "#     \"drug safety\"\n",
        "# ]\n",
        "\n",
        "# conference_urls = [\n",
        "#\n",
        "# ]\n",
        "\n",
        "# leads = scrape_pubmed_and_conferences(\n",
        "#     job_titles,\n",
        "#     publication_keywords,\n",
        "#     conference_urls\n",
        "# )\n",
        "\n",
        "# print(leads)\n"
      ],
      "metadata": {
        "id": "V1fsMgHn16Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "HUNTER_API_KEY = \"your_hunter_api_key\"\n",
        "\n",
        "def enrich_data(email_domain):\n",
        "    url = f\"https://api.hunter.io/v2/domain-search?domain={email_domain}&api_key={HUNTER_API_KEY}\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if 'data' in data:\n",
        "        emails = [entry['value'] for entry in data['data']['emails']]\n",
        "        return emails\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "email_domain = \"pfizer.com\"\n",
        "emails = enrich_data(email_domain)\n",
        "print(emails)\n"
      ],
      "metadata": {
        "id": "81giZFavyog_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_leads(leads):\n",
        "    for lead in leads:\n",
        "        score = 0\n",
        "\n",
        "        if 'Toxicology' in lead['title'] or 'Safety' in lead['title']:\n",
        "            score += 30\n",
        "\n",
        "        if lead.get('company_funding') == \"Series B\":\n",
        "            score += 20\n",
        "\n",
        "        if lead.get('published_paper'):\n",
        "            score += 40\n",
        "\n",
        "        lead['score'] = score\n",
        "\n",
        "    sorted_leads = sorted(leads, key=lambda x: x['score'], reverse=True)\n",
        "    return sorted_leads\n"
      ],
      "metadata": {
        "id": "ICyYRZOsywn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "def display_dashboard(leads):\n",
        "    df = pd.DataFrame(leads)\n",
        "    st.title(\"Lead Generation Dashboard\")\n",
        "\n",
        "\n",
        "    st.write(\"### Ranked Leads\")\n",
        "    st.dataframe(df)\n",
        "\n",
        "\n",
        "    search_query = st.text_input(\"Search for a lead\")\n",
        "    if search_query:\n",
        "        filtered_leads = df[df['name'].str.contains(search_query, case=False)]\n",
        "        st.write(filtered_leads)\n",
        "\n",
        "leads_data = [\n",
        "    {\"name\": \"Dr. Jane Smith\", \"title\": \"Director of Toxicology\", \"company\": \"Pfizer\", \"location\": \"Boston\", \"score\": 70, \"company_funding\": \"Series B\", \"published_paper\": True},\n",
        "    {\"name\": \"Dr. John Doe\", \"title\": \"Junior Scientist\", \"company\": \"XYZ Biotech\", \"location\": \"Remote\", \"score\": 30, \"company_funding\": \"Seed\", \"published_paper\": False},\n",
        "]\n",
        "\n",
        "ranked_leads = rank_leads(leads_data)\n",
        "\n",
        "display_dashboard(ranked_leads)\n"
      ],
      "metadata": {
        "id": "xpEy0vv1yzkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proper Ver"
      ],
      "metadata": {
        "id": "81AHLWD04FyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from Bio import Entrez\n",
        "import re\n",
        "\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "\n",
        "\n",
        "def scrape_all_sources(search_query, job_titles, publication_keywords, conference_urls, max_results=20):\n",
        "    leads = []\n",
        "\n",
        "    linkedin_url = f\"https://www.linkedin.com/search/results/people/?keywords={search_query}\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(linkedin_url, headers=headers, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            names = soup.find_all(\"span\", class_=\"name\")\n",
        "            titles = soup.find_all(\"div\", class_=\"job-title\")\n",
        "\n",
        "            for name, title in zip(names, titles):\n",
        "                leads.append({\n",
        "                    \"name\": name.text.strip(),\n",
        "                    \"title\": title.text.strip(),\n",
        "                    \"company\": \"\",\n",
        "                    \"source\": \"linkedin\",\n",
        "                    \"publication\": None,\n",
        "                    \"email\": None,\n",
        "                    \"phone\": None\n",
        "                })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "    query = \" AND \".join(publication_keywords)\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "\n",
        "    if record[\"IdList\"]:\n",
        "        fetch = Entrez.efetch(\n",
        "            db=\"pubmed\",\n",
        "            id=\",\".join(record[\"IdList\"]),\n",
        "            rettype=\"medline\",\n",
        "            retmode=\"text\"\n",
        "        )\n",
        "\n",
        "        articles = fetch.read().split(\"\\n\\n\")\n",
        "\n",
        "        for article in articles:\n",
        "            author = re.search(r\"AU  - (.+)\", article)\n",
        "            title = re.search(r\"TI  - (.+)\", article)\n",
        "            affiliation = re.search(r\"AD  - (.+)\", article)\n",
        "\n",
        "            if not author or not title:\n",
        "                continue\n",
        "\n",
        "            aff_text = affiliation.group(1) if affiliation else \"\"\n",
        "\n",
        "            if any(jt.lower() in aff_text.lower() for jt in job_titles):\n",
        "                leads.append({\n",
        "                    \"name\": author.group(1),\n",
        "                    \"title\": next((jt for jt in job_titles if jt.lower() in aff_text.lower()), \"Researcher\"),\n",
        "                    \"company\": aff_text,\n",
        "                    \"source\": \"pubmed\",\n",
        "                    \"publication\": title.group(1),\n",
        "                    \"email\": None,\n",
        "                    \"phone\": None\n",
        "                })\n",
        "\n",
        "    for url in conference_urls:\n",
        "        try:\n",
        "            res = requests.get(url, timeout=10)\n",
        "            if res.status_code != 200:\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "            blocks = soup.find_all(\n",
        "                [\"div\", \"li\", \"section\"],\n",
        "                class_=re.compile(\"speaker|author|faculty|presenter\", re.I)\n",
        "            )\n",
        "\n",
        "            for block in blocks:\n",
        "                text = block.get_text(\" \", strip=True)\n",
        "\n",
        "                if any(jt.lower() in text.lower() for jt in job_titles):\n",
        "                    leads.append({\n",
        "                        \"name\": text.split(\",\")[0],\n",
        "                        \"title\": next((jt for jt in job_titles if jt.lower() in text.lower()), \"\"),\n",
        "                        \"company\": \"\",\n",
        "                        \"source\": \"conference\",\n",
        "                        \"publication\": soup.title.text if soup.title else \"\",\n",
        "                        \"email\": None,\n",
        "                        \"phone\": None\n",
        "                    })\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return leads\n"
      ],
      "metadata": {
        "id": "cZbKgDqq4IXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HUNTER_API_KEY = \"your_hunter_api_key\"\n",
        "\n",
        "def enrich_lead_contacts(leads):\n",
        "    for lead in leads:\n",
        "        company_text = lead.get(\"company\", \"\")\n",
        "        domain_match = re.search(r'([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', company_text)\n",
        "\n",
        "        if not domain_match:\n",
        "            continue\n",
        "\n",
        "        domain = domain_match.group(1)\n",
        "        url = f\"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={HUNTER_API_KEY}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            continue\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if \"data\" in data and data[\"data\"][\"emails\"]:\n",
        "            lead[\"email\"] = data[\"data\"][\"emails\"][0][\"value\"]\n",
        "            lead[\"phone\"] = data[\"data\"][\"emails\"][0].get(\"phone_number\")\n",
        "\n",
        "    return leads\n"
      ],
      "metadata": {
        "id": "DMp05vZX4Ken"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_leads(leads):\n",
        "    for lead in leads:\n",
        "        score = 0\n",
        "\n",
        "        if lead[\"title\"] and \"toxicology\" in lead[\"title\"].lower():\n",
        "            score += 30\n",
        "\n",
        "        if lead[\"email\"]:\n",
        "            score += 25\n",
        "\n",
        "        if lead[\"source\"] == \"pubmed\":\n",
        "            score += 25\n",
        "\n",
        "        if lead[\"source\"] == \"conference\":\n",
        "            score += 20\n",
        "\n",
        "        lead[\"score\"] = score\n",
        "\n",
        "    return sorted(leads, key=lambda x: x[\"score\"], reverse=True)\n"
      ],
      "metadata": {
        "id": "evohc_o04Pwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lead_pipeline():\n",
        "    search_query = \"Director of Toxicology\"\n",
        "    job_titles = [\"Director\", \"Professor\", \"Principal Scientist\"]\n",
        "    publication_keywords = [\"toxicology\", \"drug safety\"]\n",
        "    conference_urls = [\"https://example-conference.org/speakers\"]\n",
        "\n",
        "    leads = scrape_all_sources(\n",
        "        search_query,\n",
        "        job_titles,\n",
        "        publication_keywords,\n",
        "        conference_urls\n",
        "    )\n",
        "\n",
        "\n",
        "    leads = enrich_lead_contacts(leads)\n",
        "\n",
        "\n",
        "    ranked_leads = rank_leads(leads)\n",
        "\n",
        "    return ranked_leads\n"
      ],
      "metadata": {
        "id": "tnLqKlb54TpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit pandas\n"
      ],
      "metadata": {
        "id": "Xlg3aILZ4t8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APP.PY\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def display_scoreboard(ranked_leads):\n",
        "    st.set_page_config(page_title=\"Lead Scoreboard\", layout=\"wide\")\n",
        "\n",
        "    st.title(\"Lead Generation Scoreboard\")\n",
        "\n",
        "    if not ranked_leads:\n",
        "        st.warning(\"No leads found.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(ranked_leads)\n",
        "\n",
        "    desired_columns = [\n",
        "        \"name\",\n",
        "        \"title\",\n",
        "        \"email\",\n",
        "        \"phone\",\n",
        "        \"company\",\n",
        "        \"source\",\n",
        "        \"score\"\n",
        "    ]\n",
        "\n",
        "    df = df[[col for col in desired_columns if col in df.columns]]\n",
        "\n",
        "\n",
        "    df = df.sort_values(by=\"score\", ascending=False)\n",
        "\n",
        "\n",
        "    search = st.text_input(\"Search by name, title, or company\")\n",
        "\n",
        "    if search:\n",
        "        df = df[df.apply(\n",
        "            lambda row: search.lower() in row.astype(str).str.lower().to_string(),\n",
        "            axis=1\n",
        "        )]\n",
        "\n",
        "\n",
        "    st.subheader(\"Ranked Leads\")\n",
        "    st.dataframe(\n",
        "        df,\n",
        "        use_container_width=True,\n",
        "        hide_index=True\n",
        "    )\n",
        "\n",
        "\n",
        "    csv = df.to_csv(index=False).encode(\"utf-8\")\n",
        "    st.download_button(\n",
        "        label=\"Download Scoreboard as CSV\",\n",
        "        data=csv,\n",
        "        file_name=\"ranked_leads_scoreboard.csv\",\n",
        "        mime=\"text/csv\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from backend_pipeline import run_lead_pipeline\n",
        "\n",
        "ranked_leads = run_lead_pipeline()\n",
        "\n",
        "display_scoreboard(ranked_leads)\n"
      ],
      "metadata": {
        "id": "UqIErtPD4zyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run the app: streamlit run app.py\n"
      ],
      "metadata": {
        "id": "foyCeN-55SgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another"
      ],
      "metadata": {
        "id": "1td50XHL6NTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from Bio import Entrez\n",
        "import re\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "Entrez.email = \"your_email@example.com\"\n",
        "HUNTER_API_KEY = \"your_hunter_api_key\"\n",
        "\n",
        "\n",
        "def scrape_all_sources(\n",
        "    search_query,\n",
        "    job_titles,\n",
        "    publication_keywords,\n",
        "    conference_urls,\n",
        "    max_results=20\n",
        "):\n",
        "    leads = []\n",
        "\n",
        "\n",
        "    linkedin_url = f\"https://www.linkedin.com/search/results/people/?keywords={search_query}\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    try:\n",
        "        res = requests.get(linkedin_url, headers=headers, timeout=10)\n",
        "        if res.status_code == 200:\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            names = soup.find_all(\"span\", class_=\"name\")\n",
        "            titles = soup.find_all(\"div\", class_=\"job-title\")\n",
        "            locations = soup.find_all(\"span\", class_=\"location\")\n",
        "\n",
        "            for name, title, loc in zip(names, titles, locations):\n",
        "                leads.append({\n",
        "                    \"name\": name.text.strip(),\n",
        "                    \"title\": title.text.strip(),\n",
        "                    \"company\": \"\",\n",
        "                    \"location\": loc.text.strip(),\n",
        "                    \"email\": None,\n",
        "                    \"phone\": None,\n",
        "                    \"source\": \"linkedin\",\n",
        "                    \"publication\": None\n",
        "                })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "    query = \" AND \".join(publication_keywords)\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "\n",
        "    if record[\"IdList\"]:\n",
        "        fetch = Entrez.efetch(\n",
        "            db=\"pubmed\",\n",
        "            id=\",\".join(record[\"IdList\"]),\n",
        "            rettype=\"medline\",\n",
        "            retmode=\"text\"\n",
        "        )\n",
        "        articles = fetch.read().split(\"\\n\\n\")\n",
        "\n",
        "        for article in articles:\n",
        "            author = re.search(r\"AU  - (.+)\", article)\n",
        "            title = re.search(r\"TI  - (.+)\", article)\n",
        "            affiliation = re.search(r\"AD  - (.+)\", article)\n",
        "\n",
        "            if not author or not title:\n",
        "                continue\n",
        "\n",
        "            aff_text = affiliation.group(1) if affiliation else \"\"\n",
        "\n",
        "            location_match = re.search(r',\\s*([^,]+,\\s*[A-Z]{2}|[^,]+,\\s*[^,]+)$', aff_text)\n",
        "            location = location_match.group(1) if location_match else \"\"\n",
        "\n",
        "            if any(jt.lower() in aff_text.lower() for jt in job_titles):\n",
        "                leads.append({\n",
        "                    \"name\": author.group(1),\n",
        "                    \"title\": next((jt for jt in job_titles if jt.lower() in aff_text.lower()), \"Researcher\"),\n",
        "                    \"company\": aff_text,\n",
        "                    \"location\": location,\n",
        "                    \"email\": None,\n",
        "                    \"phone\": None,\n",
        "                    \"source\": \"pubmed\",\n",
        "                    \"publication\": title.group(1)\n",
        "                })\n",
        "\n",
        "    for url in conference_urls:\n",
        "        try:\n",
        "            res = requests.get(url, timeout=10)\n",
        "            if res.status_code != 200:\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            blocks = soup.find_all(\n",
        "                [\"div\", \"li\", \"section\"],\n",
        "                class_=re.compile(\"speaker|author|faculty|presenter\", re.I)\n",
        "            )\n",
        "\n",
        "            for block in blocks:\n",
        "                text = block.get_text(\" \", strip=True)\n",
        "                if any(jt.lower() in text.lower() for jt in job_titles):\n",
        "                    loc_match = re.search(r'\\b[A-Z][a-z]+,\\s?[A-Z]{2}\\b', text)\n",
        "                    leads.append({\n",
        "                        \"name\": text.split(\",\")[0],\n",
        "                        \"title\": next((jt for jt in job_titles if jt.lower() in text.lower()), \"\"),\n",
        "                        \"company\": \"\",\n",
        "                        \"location\": loc_match.group(0) if loc_match else \"\",\n",
        "                        \"email\": None,\n",
        "                        \"phone\": None,\n",
        "                        \"source\": \"conference\",\n",
        "                        \"publication\": soup.title.text if soup.title else \"\"\n",
        "                    })\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return leads\n",
        "\n",
        "\n",
        "def enrich_lead_contacts(leads):\n",
        "    for lead in leads:\n",
        "        if not lead.get(\"company\"):\n",
        "            continue\n",
        "\n",
        "        domain_match = re.search(r'([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', lead[\"company\"])\n",
        "        if not domain_match:\n",
        "            continue\n",
        "\n",
        "        domain = domain_match.group(1)\n",
        "        url = f\"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={HUNTER_API_KEY}\"\n",
        "\n",
        "        try:\n",
        "            res = requests.get(url)\n",
        "            if res.status_code != 200:\n",
        "                continue\n",
        "\n",
        "            data = res.json()\n",
        "            if \"data\" in data and data[\"data\"][\"emails\"]:\n",
        "                lead[\"email\"] = data[\"data\"][\"emails\"][0][\"value\"]\n",
        "                lead[\"phone\"] = data[\"data\"][\"emails\"][0].get(\"phone_number\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return leads\n",
        "\n",
        "\n",
        "def rank_leads(leads):\n",
        "    for lead in leads:\n",
        "        score = 0\n",
        "\n",
        "        if lead[\"title\"] and \"toxicology\" in lead[\"title\"].lower():\n",
        "            score += 30\n",
        "\n",
        "        if lead[\"email\"]:\n",
        "            score += 25\n",
        "\n",
        "        if lead[\"source\"] == \"pubmed\":\n",
        "            score += 25\n",
        "\n",
        "        if lead[\"location\"]:\n",
        "            score += 10\n",
        "\n",
        "        lead[\"score\"] = score\n",
        "\n",
        "    return sorted(leads, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "\n",
        "def run_lead_pipeline():\n",
        "    leads = scrape_all_sources(\n",
        "        search_query=\"Director of Toxicology\",\n",
        "        job_titles=[\"Director\", \"Professor\", \"Principal Scientist\"],\n",
        "        publication_keywords=[\"toxicology\", \"drug safety\"],\n",
        "        conference_urls=[\"https://example-conference.org/speakers\"]\n",
        "    )\n",
        "\n",
        "    leads = enrich_lead_contacts(leads)\n",
        "    leads = rank_leads(leads)\n",
        "\n",
        "    return leads\n",
        "\n",
        "def display_scoreboard(leads):\n",
        "    st.set_page_config(layout=\"wide\")\n",
        "    st.title(\"Lead Scoreboard\")\n",
        "\n",
        "    df = pd.DataFrame(leads).sort_values(\"score\", ascending=False)\n",
        "\n",
        "    columns = [\n",
        "        \"name\",\n",
        "        \"title\",\n",
        "        \"company\",\n",
        "        \"location\",\n",
        "        \"email\",\n",
        "        \"phone\",\n",
        "        \"source\",\n",
        "        \"score\"\n",
        "    ]\n",
        "\n",
        "    df = df[[c for c in columns if c in df.columns]]\n",
        "\n",
        "    st.dataframe(df, use_container_width=True, hide_index=True)\n",
        "\n",
        "    st.download_button(\n",
        "        \"Download CSV\",\n",
        "        df.to_csv(index=False),\n",
        "        \"lead_scoreboard.csv\",\n",
        "        \"text/csv\"\n",
        "    )\n",
        "\n",
        "ranked_leads = run_lead_pipeline()\n",
        "display_scoreboard(ranked_leads)\n"
      ],
      "metadata": {
        "id": "wtW_HObI6QC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "another one"
      ],
      "metadata": {
        "id": "bzGki65_DS6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_rLw8FG0ec-",
        "outputId": "400bbb3d-aeb7-4eb2-d148-c29c2c3fef05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Bio\n",
            "  Downloading bio-1.8.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from Bio) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from Bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from Bio) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from Bio) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython>=1.80->Bio) (2.0.2)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->Bio) (4.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pooch->Bio) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->Bio) (2025.11.12)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from biothings-client>=0.2.6->mygene->Bio) (0.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (0.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.15.0)\n",
            "Downloading bio-1.8.1-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.86-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.8.1 biopython-1.86 biothings-client-0.4.1 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI42wriR0smc",
        "outputId": "8c2d0bdd-9f3f-4b8d-ba09-3836226072b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from Bio import Entrez\n",
        "import re\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "Entrez.email = \"email@example.com\"\n",
        "HUNTER_API_KEY = \"hunter_api_key\"\n",
        "\n",
        "\n",
        "def scrape_social_networks(job_titles):\n",
        "\n",
        "    leads = []\n",
        "\n",
        "    leads.append({\n",
        "        \"name\": \"Dr. Jane Smith\",\n",
        "        \"title\": \"Director of Toxicology\",\n",
        "        \"company\": \"Pfizer\",\n",
        "        \"location\": \"Boston, MA\",\n",
        "        \"email\": None,\n",
        "        \"phone\": None,\n",
        "        \"source\": \"linkedin\",\n",
        "        \"tenure_months\": 24\n",
        "    })\n",
        "    leads.append({\n",
        "        \"name\": \"Dr. John Doe\",\n",
        "        \"title\": \"VP Preclinical\",\n",
        "        \"company\": \"Novartis\",\n",
        "        \"location\": \"Basel, Switzerland\",\n",
        "        \"email\": None,\n",
        "        \"phone\": None,\n",
        "        \"source\": \"linkedin\",\n",
        "        \"tenure_months\": 12\n",
        "    })\n",
        "    return leads\n",
        "\n",
        "def scrape_publications(publication_keywords, job_titles, max_results=20):\n",
        "    leads = []\n",
        "    query = \" AND \".join(publication_keywords)\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    if record[\"IdList\"]:\n",
        "        fetch = Entrez.efetch(db=\"pubmed\", id=\",\".join(record[\"IdList\"]), rettype=\"medline\", retmode=\"text\")\n",
        "        articles = fetch.read().split(\"\\n\\n\")\n",
        "        for article in articles:\n",
        "            author = re.search(r\"AU  - (.+)\", article)\n",
        "            title = re.search(r\"TI  - (.+)\", article)\n",
        "            affiliation = re.search(r\"AD  - (.+)\", article)\n",
        "            pub_date = re.search(r\"DP  - (\\d{4})\", article)\n",
        "            if author and title:\n",
        "                aff_text = affiliation.group(1) if affiliation else \"\"\n",
        "                location_match = re.search(r',\\s*([^,]+,\\s*[A-Z]{2}|[^,]+,\\s*[^,]+)$', aff_text)\n",
        "                location = location_match.group(1) if location_match else \"\"\n",
        "                if any(jt.lower() in aff_text.lower() for jt in job_titles):\n",
        "                    recency_score = 0\n",
        "                    if pub_date:\n",
        "                        year = int(pub_date.group(1))\n",
        "                        if datetime.now().year - year <= 2:\n",
        "                            recency_score = 10\n",
        "                    leads.append({\n",
        "                        \"name\": author.group(1),\n",
        "                        \"title\": next((jt for jt in job_titles if jt.lower() in aff_text.lower()), \"Researcher\"),\n",
        "                        \"company\": aff_text,\n",
        "                        \"location\": location,\n",
        "                        \"email\": None,\n",
        "                        \"phone\": None,\n",
        "                        \"source\": \"pubmed\",\n",
        "                        \"publication\": title.group(1),\n",
        "                        \"recency_score\": recency_score\n",
        "                    })\n",
        "    return leads\n",
        "\n",
        "\n",
        "def scrape_conferences(conference_urls, job_titles):\n",
        "    leads = []\n",
        "    for url in conference_urls:\n",
        "\n",
        "        leads.append({\n",
        "            \"name\": \"Dr. Alice Brown\",\n",
        "            \"title\": \"Head of Safety Assessment\",\n",
        "            \"company\": \"Gilead\",\n",
        "            \"location\": \"San Francisco, CA\",\n",
        "            \"email\": None,\n",
        "            \"phone\": None,\n",
        "            \"source\": \"conference\",\n",
        "            \"publication\": \"AACR 2024 Speaker\",\n",
        "        })\n",
        "    return leads\n",
        "\n",
        "def enrich_budget_info(leads):\n",
        "\n",
        "    for lead in leads:\n",
        "        if \"Pfizer\" in lead.get(\"company\", \"\"):\n",
        "            lead[\"funding_score\"] = 20\n",
        "        else:\n",
        "            lead[\"funding_score\"] = 10\n",
        "    return leads\n",
        "\n",
        "\n",
        "def enrich_contacts(leads):\n",
        "    for lead in leads:\n",
        "        if lead.get(\"company\"):\n",
        "\n",
        "            lead[\"email\"] = f\"{lead['name'].split()[0].lower()}@{lead['company'].split()[0].lower()}.com\"\n",
        "            lead[\"phone\"] = \"555-123-4567\"\n",
        "    return leads\n",
        "\n",
        "\n",
        "def rank_leads(leads):\n",
        "    for lead in leads:\n",
        "        score = 0\n",
        "\n",
        "        if lead[\"title\"]:\n",
        "            if \"director\" in lead[\"title\"].lower(): score += 30\n",
        "            elif \"vp\" in lead[\"title\"].lower(): score += 25\n",
        "            elif \"head\" in lead[\"title\"].lower(): score += 20\n",
        "\n",
        "        if lead.get(\"email\"): score += 25\n",
        "\n",
        "        if lead.get(\"source\") == \"pubmed\": score += 25\n",
        "\n",
        "        score += lead.get(\"recency_score\", 0)\n",
        "\n",
        "        if lead.get(\"source\") == \"conference\": score += 15\n",
        "\n",
        "        score += lead.get(\"funding_score\", 0)\n",
        "\n",
        "        if lead.get(\"location\"): score += 10\n",
        "        lead[\"score\"] = score\n",
        "    return sorted(leads, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "\n",
        "def display_scoreboard(leads):\n",
        "    st.set_page_config(layout=\"wide\")\n",
        "    st.title(\"High-Intent Lead Scoreboard\")\n",
        "    df = pd.DataFrame(leads)\n",
        "    df = df[[\"name\",\"title\",\"company\",\"location\",\"email\",\"phone\",\"source\",\"score\"]].sort_values(\"score\", ascending=False)\n",
        "    st.dataframe(df, use_container_width=True, hide_index=True)\n",
        "    st.download_button(\n",
        "        \"Download CSV\",\n",
        "        df.to_csv(index=False),\n",
        "        \"lead_scoreboard.csv\",\n",
        "        \"text/csv\"\n",
        "    )\n",
        "\n",
        "def run_pipeline():\n",
        "    job_titles = [\"Director\", \"VP\", \"Head\"]\n",
        "    publication_keywords = [\"Drug-Induced Liver Injury\", \"Organ-on-chip\", \"Hepatic spheroids\"]\n",
        "    conference_urls = [\"https://example-conference.org/speakers\"]\n",
        "\n",
        "    leads = []\n",
        "    leads += scrape_social_networks(job_titles)\n",
        "    leads += scrape_publications(publication_keywords, job_titles)\n",
        "    leads += scrape_conferences(conference_urls, job_titles)\n",
        "    leads = enrich_budget_info(leads)\n",
        "    leads = enrich_contacts(leads)\n",
        "    leads = rank_leads(leads)\n",
        "    return leads\n",
        "\n",
        "ranked_leads = run_pipeline()\n",
        "display_scoreboard(ranked_leads)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW8kSDf0DUuY",
        "outputId": "46647987-35fb-4b0e-dc2c-33e0740cc163"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-16 15:41:58.888 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:58.889 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.094 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-12-16 15:41:59.095 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.097 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.121 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-12-16 15:41:59.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.178 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-16 15:41:59.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsTMu7X6080g",
        "outputId": "98a8741a-fc33-4ac6-b0df-4a9b442250d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.50.184.81:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo3sNImv5fYT",
        "outputId": "e0c6e9a7-16ac-4327-eedc-35fb59266d29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "F3OGEeGe5y8v",
        "outputId": "7f5a3a39-9a72-48b6-df35-1eb421ed234b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-12-16T16:03:52+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-12-16T16:03:52+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-12-16T16:03:52+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-12-16T16:03:52+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2072711294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create public tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🌐 Streamlit URL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install cloudflared\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysqtnC8s6Exf",
        "outputId": "b35fd73e-8eb6-4e12-fe37-6cad83898ab4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package cloudflared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNdLikQQ6HlK",
        "outputId": "292bdd49-f8b5-4e02-8b82-a8a2df5b4aea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "2025-12-16 16:05:17.249 Port 8501 is already in use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cloudflared tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTl2IHBi6J45",
        "outputId": "b12ad9fa-1f2c-49ea-f8da-34b3cc8e41e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cloudflared: command not found\n"
          ]
        }
      ]
    }
  ]
}